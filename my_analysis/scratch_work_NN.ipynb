{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57920d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "# Note that these go in order from bottom, side, supplemental side\n",
    "# from tof_alpha_source import *\n",
    "# PMT_x_locations = bottom_PMTs_x + side_PMTs_x + supp_side_PMTs_x\n",
    "# PMT_y_locations = bottom_PMTs_y + side_PMTs_y + supp_side_PMTs_y\n",
    "# PMT_z_locations = bottom_PMTs_z + side_PMTs_z + supp_side_PMTs_z\n",
    "# PMT_keys = bottom_PMT_list + side_PMT_list + supp_side_PMT_list\n",
    "# PMT_location_dict = {\n",
    "#     PMT_keys[i]: (PMT_x_locations[i], PMT_y_locations[i], PMT_z_locations[i])\n",
    "#     for i in range(len(PMT_keys))\n",
    "# }\n",
    "IRRELEVANT_CHANNELS = [\n",
    "    \"adc_b1_ch0\",\n",
    "    \"adc_b2_ch15\",\n",
    "    \"adc_b4_ch12\",\n",
    "    \"adc_b4_ch13\",\n",
    "    \"adc_b4_ch14\",\n",
    "    \"adc_b4_ch15\",\n",
    "]\n",
    "PMT_location_dict = {\n",
    "    \"adc_b1_ch1\": (381.0, -171.45, -677.1),\n",
    "    \"adc_b1_ch2\": (381.0, -57.15, -677.1),\n",
    "    \"adc_b1_ch3\": (381.0, 57.15, -677.1),\n",
    "    \"adc_b1_ch4\": (381.0, 171.45, -677.1),\n",
    "    \"adc_b1_ch5\": (190.5, -342.9, -677.1),\n",
    "    \"adc_b1_ch6\": (190.5, -228.6, -677.1),\n",
    "    \"adc_b1_ch7\": (190.5, -114.3, -677.1),\n",
    "    \"adc_b1_ch8\": (190.5, 0.0, -677.1),\n",
    "    \"adc_b1_ch9\": (190.5, 114.3, -677.1),\n",
    "    \"adc_b1_ch10\": (190.5, 228.6, -677.1),\n",
    "    \"adc_b1_ch11\": (190.5, 342.9, -677.1),\n",
    "    \"adc_b1_ch12\": (0.0, -400.05, -677.1),\n",
    "    \"adc_b1_ch13\": (0.0, -285.75, -677.1),\n",
    "    \"adc_b1_ch14\": (0.0, -171.45, -677.1),\n",
    "    \"adc_b1_ch15\": (0.0, -57.15, -677.1),\n",
    "    \"adc_b2_ch0\": (0.0, 57.15, -677.1),\n",
    "    \"adc_b2_ch1\": (0.0, 171.45, -677.1),\n",
    "    \"adc_b2_ch2\": (0.0, 285.75, -677.1),\n",
    "    \"adc_b2_ch3\": (0.0, 400.05, -677.1),\n",
    "    \"adc_b2_ch4\": (-190.5, -342.9, -677.1),\n",
    "    \"adc_b2_ch5\": (-190.5, -228.6, -677.1),\n",
    "    \"adc_b2_ch6\": (-190.5, -114.3, -677.1),\n",
    "    \"adc_b2_ch7\": (-190.5, 0.0, -677.1),\n",
    "    \"adc_b2_ch8\": (-190.5, 114.3, -677.1),\n",
    "    \"adc_b2_ch9\": (-190.5, 228.6, -677.1),\n",
    "    \"adc_b2_ch10\": (-190.5, 342.9, -677.1),\n",
    "    \"adc_b2_ch11\": (-381.0, -171.45, -677.1),\n",
    "    \"adc_b2_ch12\": (-381.0, -57.15, -677.1),\n",
    "    \"adc_b2_ch13\": (-381.0, 57.15, -677.1),\n",
    "    \"adc_b2_ch14\": (-381.0, 171.45, -677.1),\n",
    "    \"adc_b3_ch0\": (-532.955, 0.0, -495.3),\n",
    "    \"adc_b3_ch1\": (-532.955, 0.0, -336.55),\n",
    "    \"adc_b3_ch2\": (-532.955, 0.0, 222.25),\n",
    "    \"adc_b3_ch3\": (-532.955, 0.0, 393.7),\n",
    "    \"adc_b3_ch4\": (532.955, 0.0, -495.3),\n",
    "    \"adc_b3_ch5\": (532.955, 0.0, -336.55),\n",
    "    \"adc_b3_ch6\": (532.955, 0.0, 222.25),\n",
    "    \"adc_b3_ch7\": (532.955, 0.0, 393.7),\n",
    "    \"adc_b3_ch8\": (0.0, -532.955, -495.3),\n",
    "    \"adc_b3_ch9\": (0.0, -532.955, -336.55),\n",
    "    \"adc_b3_ch10\": (0.0, -532.955, 222.25),\n",
    "    \"adc_b3_ch11\": (0.0, -532.955, 393.7),\n",
    "    \"adc_b3_ch12\": (0.0, 532.955, -495.3),\n",
    "    \"adc_b3_ch13\": (0.0, 532.955, -336.55),\n",
    "    \"adc_b3_ch14\": (0.0, 532.955, 222.25),\n",
    "    \"adc_b3_ch15\": (0.0, 532.955, 393.7),\n",
    "    \"adc_b4_ch0\": (-376.8561, -376.8561, -211.0232),\n",
    "    \"adc_b4_ch1\": (-376.8561, -376.8561, -41.1607),\n",
    "    \"adc_b4_ch2\": (-376.8561, -376.8561, 128.7018),\n",
    "    \"adc_b4_ch3\": (376.8561, 376.8561, -211.0232),\n",
    "    \"adc_b4_ch4\": (376.8561, 376.8561, -41.1607),\n",
    "    \"adc_b4_ch5\": (376.8561, 376.8561, 128.7018),\n",
    "    \"adc_b4_ch6\": (376.8561, -376.8561, -211.0232),\n",
    "    \"adc_b4_ch7\": (376.8561, -376.8561, -41.1607),\n",
    "    \"adc_b4_ch8\": (376.8561, -376.8561, 128.7018),\n",
    "    \"adc_b4_ch9\": (-376.8561, 376.8561, -211.0232),\n",
    "    \"adc_b4_ch10\": (-376.8561, 376.8561, -41.1607),\n",
    "    \"adc_b4_ch11\": (-376.8561, 376.8561, 128.7018),\n",
    "}\n",
    "\n",
    "# temporarily using a low statistics one here\n",
    "PMT_channel_delay_dict = {\n",
    "    \"adc_b1_ch1\": np.float64(7.8599166820699455),\n",
    "    \"adc_b1_ch2\": np.float64(4.2279287488665505),\n",
    "    \"adc_b1_ch3\": np.float64(5.574034508625212),\n",
    "    \"adc_b1_ch4\": np.float64(1.462657718896561),\n",
    "    \"adc_b1_ch5\": np.float64(5.46018892397303),\n",
    "    \"adc_b1_ch6\": np.float64(5.409302133389168),\n",
    "    \"adc_b1_ch7\": np.float64(3.9025018950943133),\n",
    "    \"adc_b1_ch8\": np.float64(3.46381735976686),\n",
    "    \"adc_b1_ch9\": np.float64(3.885940297701321),\n",
    "    \"adc_b1_ch10\": np.float64(4.053950701931467),\n",
    "    \"adc_b1_ch11\": np.float64(5.574917569091319),\n",
    "    \"adc_b1_ch12\": np.float64(3.4946922002103027),\n",
    "    \"adc_b1_ch13\": np.float64(2.660567554458799),\n",
    "    \"adc_b1_ch14\": np.float64(3.9224651796362338),\n",
    "    \"adc_b1_ch15\": np.float64(4.611150134965591),\n",
    "    \"adc_b2_ch0\": np.float64(5.657053247743757),\n",
    "    \"adc_b2_ch1\": np.float64(3.9912637690502324),\n",
    "    \"adc_b2_ch2\": np.float64(4.004392700853049),\n",
    "    \"adc_b2_ch3\": np.float64(3.5197659369335343),\n",
    "    \"adc_b2_ch4\": np.float64(2.4646066520412377),\n",
    "    \"adc_b2_ch5\": np.float64(3.59695668842358),\n",
    "    \"adc_b2_ch6\": np.float64(6.1581988684208735),\n",
    "    \"adc_b2_ch7\": np.float64(4.4218622173912525),\n",
    "    \"adc_b2_ch8\": np.float64(0.6521201336279718),\n",
    "    \"adc_b2_ch9\": np.float64(4.226312224499658),\n",
    "    \"adc_b2_ch10\": np.float64(5.533503195204535),\n",
    "    \"adc_b2_ch11\": np.float64(5.29138682467591),\n",
    "    \"adc_b2_ch12\": np.float64(4.0094185548337125),\n",
    "    \"adc_b2_ch13\": np.float64(4.08831536665447),\n",
    "    \"adc_b2_ch14\": np.float64(2.5962437226528117),\n",
    "    \"adc_b3_ch0\": np.float64(3.442589791305678),\n",
    "    \"adc_b3_ch1\": np.float64(1.6667315660917326),\n",
    "    \"adc_b3_ch2\": np.float64(3.423351974027864),\n",
    "    \"adc_b3_ch3\": np.float64(6.209936881382594),\n",
    "    \"adc_b3_ch4\": np.float64(1.3518561473109876),\n",
    "    \"adc_b3_ch5\": np.float64(1.8722080052467223),\n",
    "    \"adc_b3_ch6\": np.float64(4.0035516253739),\n",
    "    \"adc_b3_ch7\": np.float64(3.2043085739745996),\n",
    "    \"adc_b3_ch8\": np.float64(3.8311094721486803),\n",
    "    \"adc_b3_ch9\": np.float64(4.5314879480281345),\n",
    "    \"adc_b3_ch10\": np.float64(2.434797802550851),\n",
    "    \"adc_b3_ch11\": np.float64(3.767981411743301),\n",
    "    \"adc_b3_ch12\": np.float64(2.396616695331079),\n",
    "    \"adc_b3_ch13\": np.float64(3.4128794605255703),\n",
    "    \"adc_b3_ch14\": np.float64(3.246248631680017),\n",
    "    \"adc_b3_ch15\": np.float64(1.4297591545820207),\n",
    "    \"adc_b4_ch0\": np.float64(2.704113081726384),\n",
    "    \"adc_b4_ch1\": np.float64(2.0354747883796236),\n",
    "    \"adc_b4_ch2\": np.float64(4.41186642937317),\n",
    "    \"adc_b4_ch3\": np.float64(3.7450287508165627),\n",
    "    \"adc_b4_ch4\": np.float64(2.039541912010301),\n",
    "    \"adc_b4_ch5\": np.float64(1.9065206786959155),\n",
    "    \"adc_b4_ch6\": np.float64(2.7356183903993316),\n",
    "    \"adc_b4_ch7\": np.float64(3.821243571830856),\n",
    "    \"adc_b4_ch8\": np.float64(10.311279529931609),\n",
    "    \"adc_b4_ch9\": np.float64(2.8341494076225557),\n",
    "    \"adc_b4_ch10\": np.float64(4.047511889036665),\n",
    "    \"adc_b4_ch11\": np.float64(4.251758108079468),\n",
    "}\n",
    "\n",
    "def get_1t_info(fname: str):\n",
    "    f = uproot.open(fname)\n",
    "    daq = f[\"daq\"]\n",
    "    # sometimes this isn't in the root file\n",
    "    if \"run_info\" in f:\n",
    "        run_info = f[\"run_info\"]\n",
    "    else:\n",
    "        run_info = None\n",
    "    daqkeys = daq.keys()\n",
    "    traces = {}\n",
    "    for key in daq.keys():\n",
    "        if \"adc\" in key:\n",
    "            traces[key] = daq[key].array(library=\"np\")\n",
    "    event_ttt1 = daq[\"event_ttt_1\"].array(library=\"np\")\n",
    "    event_ttt2 = daq[\"event_ttt_2\"].array(library=\"np\")\n",
    "    event_ttt3 = daq[\"event_ttt_3\"].array(library=\"np\")\n",
    "    event_ttt4 = daq[\"event_ttt_4\"].array(library=\"np\")\n",
    "    event_ttt5 = daq[\"event_ttt_5\"].array(library=\"np\")\n",
    "    event_id = daq[\"event_id\"].array(library=\"np\")\n",
    "    event_sanity = daq[\"event_sanity\"].array(library=\"np\")\n",
    "    # event_ttt = daq['event_ttt'].array(library='np')\n",
    "\n",
    "    return (\n",
    "        traces,\n",
    "        event_ttt1.astype(np.int64),\n",
    "        event_ttt2.astype(np.int64),\n",
    "        event_ttt3.astype(np.int64),\n",
    "        event_ttt4.astype(np.int64),\n",
    "        event_ttt5.astype(np.int64),\n",
    "        event_id,\n",
    "        event_sanity,\n",
    "        daqkeys,\n",
    "        run_info,\n",
    "    )\n",
    "\n",
    "def base_and_flip(waveform):\n",
    "    \"\"\"Subtract baseline and reflect over x axis\"\"\"\n",
    "    positive_waveform = (waveform - np.median(waveform)) * (-1)\n",
    "    return positive_waveform\n",
    "\n",
    "def weighted_average_hit_time(waveform, window_size=10):\n",
    "    \"\"\"Do weighted average in window around pulse. Returns float value at which\n",
    "    hit time occurred\"\"\"\n",
    "    # Call correction algorithm\n",
    "    waveform = base_and_flip(waveform)\n",
    "\n",
    "    # Make into list\n",
    "    waveform = list(waveform)\n",
    "    \n",
    "    # Find index of max (the pulse peak)\n",
    "    peak_index = waveform.index(max(waveform))\n",
    "    \n",
    "    # Define window bounds\n",
    "    half_window = window_size // 2\n",
    "    start = max(0, peak_index - half_window)\n",
    "    end = min(len(waveform), peak_index + half_window + 1)\n",
    "    \n",
    "    # Get time (index) and amplitude (value) in the window\n",
    "    times = list(range(start, end))\n",
    "    amplitudes = waveform[start:end]\n",
    "    \n",
    "    # Compute weighted average hit time\n",
    "    numerator = sum(t * a for t, a in zip(times, amplitudes))\n",
    "    denominator = sum(amplitudes)\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return None  # Avoid divide-by-zero\n",
    "    return numerator / denominator\n",
    "\n",
    "def get_channel_charge(waveform):\n",
    "    \"\"\"Takes in a raw waveform. Does baseline subtraction, makes it positive, make window of\n",
    "    size 60ns / 30 sample, integrate by just taking sum (nothing fancy), divide by 50 (resistance),\n",
    "    returns charge in pC\"\"\"\n",
    "    based_flipped = base_and_flip(waveform)\n",
    "    time_of_max = np.argmax(based_flipped)\n",
    "    charge_pC = np.sum(based_flipped[time_of_max - 5: time_of_max + 5]) / 50\n",
    "    return charge_pC\n",
    "\n",
    "def waveform_daisy_correction(waveform, boardID):\n",
    "    if (boardID < 1) or (boardID > 4):\n",
    "        print(\"Bad BoardID\")\n",
    "        return False\n",
    "    elif boardID != 1:\n",
    "        return waveform[24 * (4 - boardID) : -24 * (boardID - 1)]\n",
    "    else:\n",
    "        return waveform[24 * 3 :]\n",
    "\n",
    "def is_pulse(waveform: np.ndarray, range_min: int = 0, range_max: int = 1928) -> bool:\n",
    "    \"\"\"Takes in a daisy corrected waveform and looks in a given range to see if there is a pulse.\n",
    "    For example, you can use some range around an alpha PMT hit if looking for just alpha detections.\n",
    "    \"\"\"\n",
    "    threshold_sigma = 20\n",
    "    # this is arbitrary, and hopefully sufficient\n",
    "    baseline = np.median(waveform[:int(0.5 * len(waveform))])\n",
    "    noise_std = np.std(waveform[:int(0.5 * len(waveform))])\n",
    "    deviation = np.abs(waveform - baseline)\n",
    "    threshold = threshold_sigma * noise_std\n",
    "\n",
    "    # Get all indices where deviation exceeds threshold\n",
    "    pulse_indices = np.where(deviation > threshold)[0]\n",
    "    for i in pulse_indices:\n",
    "        if range_min <= i <= range_max:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def b4_ch13_or_ch14_detections(traces):\n",
    "    \"\"\"These are top paddle channels, gets list of events with detections.\"\"\"\n",
    "    b4_ch13_or_ch14_detection_list = []\n",
    "    waveforms_list = traces[\"adc_b4_ch13\"] + traces[\"adc_b4_ch14\"]\n",
    "    for i, waveform in enumerate(waveforms_list):\n",
    "        if is_pulse(waveform):  # this is arbitrary, and hopefully this is sufficient\n",
    "            b4_ch13_or_ch14_detection_list.append(i)\n",
    "    return b4_ch13_or_ch14_detection_list\n",
    "\n",
    "def top_paddle_event_list(traces) -> list[int]:\n",
    "    \"\"\"Returns a list of event indices that correspond to top_paddle trigger events.\n",
    "    This means b4_ch13 OR b4_ch14 has a signal AND that the superposition of signals\n",
    "    for that event lies in the time range for top_paddle events.\"\"\"\n",
    "\n",
    "    twice_checked_top_paddle_event_index_list = []\n",
    "    num_events = len(traces[\"adc_b2_ch1\"])  # pick arbitrary PMT, all same length\n",
    "    top_paddle_PMT_events = b4_ch13_or_ch14_detections(traces)  # fix for top_paddle\n",
    "\n",
    "    for i in range(num_events):\n",
    "        corrected_waveforms_per_event = []\n",
    "        # waveform loop to get the i_th waveform for each PMT\n",
    "        for key in traces.keys():\n",
    "            if (\"b5\" in key) or (key in IRRELEVANT_CHANNELS):  # Adam said disregard\n",
    "                continue\n",
    "            board_num = int(key[5])\n",
    "            uncorrected_waveform = traces[key][i]\n",
    "            corrected_waveforms_per_event.append(\n",
    "                waveform_daisy_correction(uncorrected_waveform, board_num)\n",
    "            )\n",
    "        summed_waveform = np.sum(corrected_waveforms_per_event, axis=0)\n",
    "        peak_sample_time_ns = np.argmin(summed_waveform) * 2  # converts ADU to mV\n",
    "        # rough estimate of time range\n",
    "        if 750 < peak_sample_time_ns and i in top_paddle_PMT_events:\n",
    "            twice_checked_top_paddle_event_index_list.append(i)\n",
    "    return twice_checked_top_paddle_event_index_list\n",
    "\n",
    "def get_all_sensor_input(fname: str, peak_method: str):\n",
    "    \"\"\"Takes in a file path and a peak_method, which is either CFD or weighted avg.\n",
    "    Then the information for all-sensor chargenet AND all-sensor hitnet is returned.\n",
    "    ADD IN the peak method later.\"\"\"\n",
    "\n",
    "    all_events = []\n",
    "    \n",
    "    traces = get_1t_info(fname)[0]\n",
    "    top_paddle_events = top_paddle_event_list(traces)\n",
    "\n",
    "    for i in top_paddle_events:\n",
    "        hitnet_input = [[], [], [], [], []]\n",
    "        chargenet_input = []\n",
    "        sum_of_charges_of_all_hits = 0\n",
    "        num_of_hits = 0\n",
    "        # waveform loop to get the i_th waveform for each PMT\n",
    "        for key in traces.keys():\n",
    "            if (\"b5\" in key) or (key in IRRELEVANT_CHANNELS):  # disregard\n",
    "                continue\n",
    "            uncorrected_waveform = traces[key][i]\n",
    "            board_num = int(key[5])\n",
    "            \n",
    "            # perform daisy correction and change from ADU to ns (500MHz sampling)\n",
    "            daisy_corrected_waveform = waveform_daisy_correction(\n",
    "                uncorrected_waveform, board_num\n",
    "            ) * 2\n",
    "\n",
    "            # Plot histogram\n",
    "            plt.plot(daisy_corrected_waveform)\n",
    "\n",
    "            # Add labels and title\n",
    "            plt.xlabel('x')\n",
    "            plt.ylabel('y')\n",
    "            plt.title('Waveform')\n",
    "\n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # skip if not a pulse (add in pulse window?)\n",
    "            # GET ONLY THE NECESSARY EVENTS from TP trigger!!!\n",
    "            if not is_pulse(daisy_corrected_waveform, range_min=750):\n",
    "                continue\n",
    "\n",
    "            # factors in the channel delay and reassigns it to daisy_corrected_waveform\n",
    "            daisy_corrected_waveform = [x - PMT_channel_delay_dict[key] for x in daisy_corrected_waveform]\n",
    "            \n",
    "            # Fix wonky waveforms from June 30, 2025 by ensuring charge > 0\n",
    "            # potentially not a rigorous fix\n",
    "            waveform_charge = get_channel_charge(daisy_corrected_waveform)\n",
    "            if waveform_charge < 0:\n",
    "                continue\n",
    "\n",
    "            # hitnet input as we go\n",
    "            hitnet_input[0].append(PMT_location_dict[key][0])\n",
    "            hitnet_input[1].append(PMT_location_dict[key][1])\n",
    "            hitnet_input[2].append(PMT_location_dict[key][2])\n",
    "            if peak_method == \"CFD\":\n",
    "                hitnet_input[3].append(-1)\n",
    "            elif peak_method == \"W_avg\":\n",
    "                hitnet_input[3].append(weighted_average_hit_time(daisy_corrected_waveform))\n",
    "            hitnet_input[4].append(1)\n",
    "\n",
    "            # chargenet values to later input\n",
    "            sum_of_charges_of_all_hits += waveform_charge\n",
    "            num_of_hits += 1\n",
    "\n",
    "        # chargenet input\n",
    "        chargenet_input.append(sum_of_charges_of_all_hits)\n",
    "        chargenet_input.append(num_of_hits)\n",
    "\n",
    "        event = {\n",
    "            \"hits\": np.stack(hitnet_input, axis=1),\n",
    "            \"total_charge\": np.stack(chargenet_input),\n",
    "        }\n",
    "        print(hitnet_input, \"\\n\", chargenet_input)\n",
    "\n",
    "        all_events.append(event)\n",
    "    return all_events\n",
    "\n",
    "\n",
    "\n",
    "# phase_directory = \"/media/disk_d/WbLS-DATA/raw_root/phase3/muon/\" # Oct 31, 2024\n",
    "# phase_directory = \"/media/disk_a/WbLS-DATA/raw_root/phase6/muon/\" # Jan 07, 2025\n",
    "# phase_directory = \"/media/disk_b/WbLS-DATA/raw_root/phase6/muon/\" # Dec 19, 2024\n",
    "# phase_directory = \"/media/disk_e/WbLS-DATA/raw_root/phase4/muon/\" # Dec 03, 2024\n",
    "phase_directory = \"/media/disk_k/WbLS-DATA/raw_root/phase8/muon/\" # Mar 11, 2025\n",
    "file_paths_for_ch_delays = [phase_directory + str(f) for f in os.listdir(phase_directory) if os.path.isfile(os.path.join(phase_directory, f))][:200]\n",
    "\n",
    "for fileee in file_paths_for_ch_delays:\n",
    "    print(\"starting new file\", fileee)\n",
    "    # print(get_all_sensor_input(fileee, \"W_avg\"))\n",
    "    get_all_sensor_input(fileee, \"W_avg\")\n",
    "    time.sleep(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d67ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/media/disk_o/my_pickles/processed_data_for_disk_k_phase_8_30.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(len(data[\"all_events_for_phase\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0daae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beefy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
